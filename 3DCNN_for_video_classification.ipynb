{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video Classification using 3D Convlolution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LEGION\\anaconda3\\envs\\rl\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "# importing necessary packages\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from glob import glob\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LEGION\\anaconda3\\envs\\rl\\lib\\site-packages\\torchvision\\transforms\\_functional_video.py:6: UserWarning: The 'torchvision.transforms._functional_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms.functional' module instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\LEGION\\anaconda3\\envs\\rl\\lib\\site-packages\\torchvision\\transforms\\_transforms_video.py:22: UserWarning: The 'torchvision.transforms._transforms_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms' module instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "import torchmetrics\n",
    "from torchmetrics import functional\n",
    "from torchmetrics.classification import BinaryAccuracy\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms._transforms_video import CenterCropVideo, NormalizeVideo\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import pytorchvideo\n",
    "from pytorchvideo.data.encoded_video import EncodedVideo\n",
    "\n",
    "# Augumentation Process\n",
    "from pytorchvideo.data import (\n",
    "    make_clip_sampler,\n",
    "    labeled_video_dataset,\n",
    ")\n",
    "\n",
    "from torchvision.transforms import Compose, RandomHorizontalFlip\n",
    "\n",
    "\n",
    "from torchvision.transforms import (\n",
    "    Compose,\n",
    "    RandomHorizontalFlip,\n",
    "    Resize,\n",
    ")\n",
    "\n",
    "from pytorchvideo.transforms import (\n",
    "    ApplyTransformToKey,\n",
    "    Normalize,\n",
    "    RandomShortSideScale,\n",
    "    UniformTemporalSubsample,\n",
    ")\n",
    "\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, LearningRateMonitor\n",
    "from pytorch_lightning import LightningModule, seed_everything, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "non violence video 50\n",
      "violence video 50\n"
     ]
    }
   ],
   "source": [
    "# reading the video data using glob\n",
    "non = glob(\"Dataset/Dummy/Train/NonViolence/*\")\n",
    "vio = glob(\"Dataset/Dummy/Train/Violence/*\")\n",
    "\n",
    "label = [0] * len(non) + [1] * len(vio)\n",
    "df = pd.DataFrame(zip(non + vio, label), columns=[\"file\", \"label\"])\n",
    "\n",
    "print(\"non violence video\", len(non))\n",
    "print(\"violence video\", len(vio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dataset/Dummy/Train/NonViolence\\NV_1.mp4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dataset/Dummy/Train/NonViolence\\NV_10.mp4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dataset/Dummy/Train/NonViolence\\NV_11.mp4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dataset/Dummy/Train/NonViolence\\NV_12.mp4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dataset/Dummy/Train/NonViolence\\NV_13.mp4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Dataset/Dummy/Train/Violence\\V_50.mp4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Dataset/Dummy/Train/Violence\\V_6.mp4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Dataset/Dummy/Train/Violence\\V_7.mp4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Dataset/Dummy/Train/Violence\\V_8.mp4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Dataset/Dummy/Train/Violence\\V_9.mp4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         file  label\n",
       "0    Dataset/Dummy/Train/NonViolence\\NV_1.mp4      0\n",
       "1   Dataset/Dummy/Train/NonViolence\\NV_10.mp4      0\n",
       "2   Dataset/Dummy/Train/NonViolence\\NV_11.mp4      0\n",
       "3   Dataset/Dummy/Train/NonViolence\\NV_12.mp4      0\n",
       "4   Dataset/Dummy/Train/NonViolence\\NV_13.mp4      0\n",
       "..                                        ...    ...\n",
       "95      Dataset/Dummy/Train/Violence\\V_50.mp4      1\n",
       "96       Dataset/Dummy/Train/Violence\\V_6.mp4      1\n",
       "97       Dataset/Dummy/Train/Violence\\V_7.mp4      1\n",
       "98       Dataset/Dummy/Train/Violence\\V_8.mp4      1\n",
       "99       Dataset/Dummy/Train/Violence\\V_9.mp4      1\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# displaying the dataframe\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Normalize the frame extracted from the video\n",
    "\"\"\"\n",
    "class NormalizeVideo(torch.nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResizeVideo(torch.nn.Module):\n",
    "    def __init__(self, output_size):\n",
    "        super(ResizeVideo, self).__init__()\n",
    "        self.resize = transforms.Resize(output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Assuming x is a video tensor with shape (batch_size, num_channels, num_frames, height, width)\n",
    "        batch_size, num_channels, num_frames, height, width = x.size()\n",
    "        x_reshaped = x.view(-1, num_channels, height, width)  # Reshape to (batch_size*num_frames, num_channels, height, width)\n",
    "        x_resized = self.resize(x_reshaped)  # Resize frames\n",
    "        \n",
    "        return x_resized.view(batch_size, num_channels, num_frames, x_resized.size(2), x_resized.size(3))  # Reshape back to original shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instead of mannually preparing the data pytorchvideo provide library to process the video data\n",
    "video_transform = Compose(\n",
    "    [\n",
    "        ApplyTransformToKey(\n",
    "            key=\"video\",\n",
    "            transform=Compose(\n",
    "                [\n",
    "                    # Sampling the video, for this case sampling 10 frames per second\n",
    "                    UniformTemporalSubsample(10),\n",
    "                    # Normalizing the video\n",
    "                    NormalizeVideo(),\n",
    "                    # Normalizing the frames using mean and standard deviation\n",
    "                    Normalize((0.45, 0.45, 0.45), (0.225, 0.225, 0.225)),\n",
    "                    # Performing RandomShortSideScale\n",
    "                    RandomShortSideScale(min_size=248, max_size=256),\n",
    "                    # Perfoming Center Crop\n",
    "                    CenterCropVideo(112),\n",
    "                    # Performing RandomHorizontal Flip\n",
    "                    RandomHorizontalFlip(p=0.5),\n",
    "                ]\n",
    "            ),\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dataloader for loading data, \n",
    "# Augumentation is already done when loading frames\n",
    "train_dataset = labeled_video_dataset(\n",
    "    \"./Dataset/Dummy\",\n",
    "    clip_sampler=make_clip_sampler(\"random\", 2),\n",
    "    transform=video_transform,\n",
    "    decode_audio=False,\n",
    ")\n",
    "\n",
    "# loader to load data\n",
    "loader = DataLoader(train_dataset, batch_size=5, num_workers=0, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['video', 'video_name', 'video_index', 'clip_index', 'aug_index', 'label'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking the shape of inputs\n",
    "batch = next(iter(loader))\n",
    "batch.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension of the video: torch.Size([5, 3, 10, 112, 112])\n",
      "Dimension of the label: torch.Size([5])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Dimension of the video: {batch['video'].shape}\")\n",
    "print(f\"Dimension of the label: {batch['label'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizing the video\n",
    "import cv2\n",
    "\n",
    "# For playing videos\n",
    "def play_video(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        cv2.imshow('Video', frame)\n",
    "        if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Path to your video file\n",
    "video_path = \"Dataset/Dummy/Test/Violence/V_101.mp4\"\n",
    "\n",
    "# play_video(video_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension of the video: torch.Size([5, 3, 10, 112, 112])\n",
      "Dimension of the label: torch.Size([5])\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(loader))\n",
    "\n",
    "print(f\"Dimension of the video: {batch['video'].shape}\")\n",
    "print(f\"Dimension of the label: {batch['label'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxPool3D(nn.Module):\n",
    "    def __init__(self, kernel_size: tuple, stride: tuple=None, padding: tuple=(0, 0, 0)):\n",
    "        \"\"\"\n",
    "        This function initializes the parameters for a maxpool layer\n",
    "\n",
    "        Parameters\n",
    "        ------------\n",
    "            kernel_size : tuple\n",
    "            window height and width for the maxpooling window\n",
    "\n",
    "            stride : tuple\n",
    "            the stride of the window. Default value is kernel_size\n",
    "\n",
    "            padding: int\n",
    "            implicit zero padding to be added \n",
    "        \"\"\"\n",
    "        super(MaxPool3D, self).__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = kernel_size if stride is None else stride\n",
    "        self.padding = padding\n",
    "\n",
    "\n",
    "    def forward(self, x: torch.tensor): \n",
    "        \"\"\"\n",
    "        This function performs max-pool operation on the input\n",
    "\n",
    "        Parameters\n",
    "        ------------\n",
    "            x : tensor, float32\n",
    "            Input image to the convolution layer\n",
    "\n",
    "        Returns\n",
    "        ------------\n",
    "            x : tensor, float32\n",
    "            max-pooled output from the last layer\n",
    "        \"\"\"\n",
    "        # Returning the max pool operation\n",
    "        return F.max_pool3d(x, kernel_size=self.kernel_size, stride=self.stride, padding=self.padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reshape(nn.Module):\n",
    "    \"\"\"\n",
    "    Reshape the input to target shape\n",
    "    \"\"\"\n",
    "    def __init__(self, target_shape: tuple):\n",
    "        super(Reshape, self).__init__()\n",
    "        self.target_shape = target_shape\n",
    "    \n",
    "    def forward(self, inputs: torch.tensor):\n",
    "        return inputs.reshape((-1, *self.target_shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "INPUT_DIM = (5, 3, 10, 112, 112)\n",
    "\n",
    "class Video3DCNN(nn.Module):\n",
    "    def __init__(self, num_output_features=400):\n",
    "        super(Video3DCNN, self).__init__()\n",
    "\n",
    "        self.features = nn.Sequential(\n",
    "            # first hidden layer\n",
    "            nn.Conv3d(in_channels=3, out_channels=2, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1)),\n",
    "            MaxPool3D(kernel_size=(1, 2, 2), stride=(1, 2, 2)),\n",
    "\n",
    "            # Second hidden layer\n",
    "            nn.Conv3d(in_channels=2, out_channels=4, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1)),\n",
    "            MaxPool3D(kernel_size=(2, 2, 2), stride=(2, 2, 2)),\n",
    "\n",
    "            # Third hidden layer\n",
    "            nn.Conv3d(in_channels=4, out_channels=8, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1)),\n",
    "            nn.Conv3d(in_channels=8, out_channels=8, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1)),\n",
    "            MaxPool3D(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=(1, 1, 1)),\n",
    "\n",
    "            # Fourth hidden layer\n",
    "            nn.Conv3d(in_channels=8, out_channels=16, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1)),\n",
    "            nn.Conv3d(in_channels=16, out_channels=16, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1)),\n",
    "            MaxPool3D(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=(1, 1, 1)),\n",
    "\n",
    "            # Fifth hidden layer\n",
    "            nn.Conv3d(in_channels=16, out_channels=16, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1)),\n",
    "            nn.Conv3d(in_channels=16, out_channels=16, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1)),\n",
    "            MaxPool3D(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=(1, 1, 1)),\n",
    "        )\n",
    "\n",
    "        # Compute the shape after conv layers dynamically\n",
    "        self._initialize_fc_layers(INPUT_DIM)\n",
    "\n",
    "        # Fully connected layers with the specified output sizes\n",
    "        self.fc1 = nn.Linear(in_features=self.fc_input_features, out_features=800)  # Adjusted to match next layer's input\n",
    "        self.fc2 = nn.Linear(in_features=800, out_features=1000)  # Adjusted to match next layer's input\n",
    "        self.fc3 = nn.Linear(in_features=1000, out_features=num_output_features)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "\n",
    "    def _initialize_fc_layers(self, input_shape):\n",
    "        \"\"\"\n",
    "        Used to calculate the dimension that gets passed to linear layer from the convolution layer\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            x = torch.rand(input_shape)\n",
    "            x = self.features(x)\n",
    "            self.fc_input_features = x.view(x.size(0), -1).size(1)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten the tensor\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, p=0.5)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.dropout(x, p=0.5)\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "# # Create a model instance\n",
    "# model = Video3DCNN(num_output_features=400)\n",
    "\n",
    "# # Print model summary\n",
    "# print(model)\n",
    "\n",
    "# # Dummy input: Batch of 5 videos, 3 color channels, 10 frames, 112x112 pixels\n",
    "# dummy_input = torch.randn(5, 3, 10, 112, 112)  # Adjusted the batch size to match the target data\n",
    "\n",
    "# # Forward pass\n",
    "# output = model(dummy_input)\n",
    "# print(output.shape)  # Expected output shape: (5, 400)\n",
    "\n",
    "# # Sample target for testing loss calculation\n",
    "# sample_target = torch.randint(0, 400, (5,))  # Assuming 5 samples and 400 classes, batch size matches input data\n",
    "\n",
    "# # Define a loss function\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# loss = criterion(output, sample_target)\n",
    "# print(f\"Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoClassifier(LightningModule):\n",
    "    def __init__(self):\n",
    "        super(VideoClassifier, self).__init__()\n",
    "\n",
    "        # For storing the hyperparameters\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        # For logs\n",
    "        self.training_step_loss = []\n",
    "        self.training_step_metric = []\n",
    "        self.validation_step_loss = []\n",
    "        self.validation_step_metric = []\n",
    "\n",
    "        self.validation_step_y_hats = []\n",
    "        self.validation_step_ys = []\n",
    "        \n",
    "        # Architecture\n",
    "        self.video_module = Video3DCNN()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear = nn.Linear(400, 1)\n",
    "\n",
    "        # Hyperparameters\n",
    "        self.lr = 1e-3\n",
    "        self.batch_size = 5\n",
    "        self.numworker = 3\n",
    "\n",
    "        # evaluation metric\n",
    "        self.metric = BinaryAccuracy()\n",
    "\n",
    "        # loss function\n",
    "        self.criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.video_module(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear(x)\n",
    "\n",
    "        return x.squeeze(dim=1)  # Squeeze to make predictions have shape (batch_size,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_optimizer(self):\n",
    "    \"\"\"\n",
    "    Configuration of optizer and decaying learning rate for the neural network\n",
    "    \"\"\"\n",
    "    opt = torch.optim.AdamW(params=self.parameters(), lr=self.lr)\n",
    "    scheduler = CosineAnnealingLR(opt, T_max=10, eta_min=1e-6, last_epoch=-1)\n",
    "    return {\"optimizer\": opt, \"lr_scheduler\": scheduler}\n",
    "\n",
    "VideoClassifier.configure_optimizers = configure_optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loader(path: str):\n",
    "    \"\"\"\n",
    "    Defining the data loader for the train, test and validation set\n",
    "    \"\"\"\n",
    "    dataset = labeled_video_dataset(\n",
    "        path,\n",
    "        clip_sampler=make_clip_sampler(\"random\", 1),\n",
    "        transform=video_transform,\n",
    "        decode_audio=False,\n",
    "    )\n",
    "\n",
    "    loader = DataLoader(dataset, batch_size=5, num_workers=0, pin_memory=True)\n",
    "\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dataloader(self):\n",
    "    \"\"\"\n",
    "    Train dataloader\n",
    "    \"\"\"\n",
    "    return data_loader(path=\"Dataset/Dummy/Train\")\n",
    "\n",
    "VideoClassifier.train_dataloader = train_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_step(self, batch, batch_idx):\n",
    "    \"\"\"\n",
    "    Training step for the lightning module\n",
    "    \"\"\"\n",
    "    # Extracting the data and target\n",
    "    video, label = batch[\"video\"], batch[\"label\"]\n",
    "    \n",
    "    # Forward Pass\n",
    "    out = self(video)\n",
    "\n",
    "    # Calculation of loss and metric\n",
    "    loss = self.criterion(out.squeeze(), label.float())\n",
    "    metric = self.metric(out, label.to(torch.int64))\n",
    "\n",
    "    # Logging the loss\n",
    "    self.log(\"train_loss\", loss)\n",
    "\n",
    "    self.training_step_loss.append(loss)\n",
    "    self.training_step_metric.append(metric)\n",
    "\n",
    "    return {\"loss\": loss, \"metric\": metric.detach()}\n",
    "\n",
    "VideoClassifier.training_step = training_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_train_epoch_end(self, *args, **kwargs):\n",
    "    \"\"\"\n",
    "    Whenever the epoch ends, this function runs.\n",
    "    \"\"\"\n",
    "    epoch_avg_loss = torch.stack(self.training_step_loss).mean().detach().cpu().numpy()\n",
    "    epoch_avg_metric = torch.stack(self.training_step_metric).mean().detach().cpu().numpy()\n",
    "\n",
    "    self.log(\"Training Loss\", torch.from_numpy(epoch_avg_loss).type(torch.float32))\n",
    "    self.log(\"Training Metric\", torch.from_numpy(epoch_avg_metric).type(torch.float32))\n",
    "\n",
    "    # self.training_step_loss.clear()\n",
    "    # self.training_step_metric.clear()\n",
    "\n",
    "VideoClassifier.on_train_epoch_end = on_train_epoch_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_dataloader(self):\n",
    "    \"\"\"\n",
    "    loader for validation set.\n",
    "    \"\"\"\n",
    "    return data_loader(path=\"Dataset/Dummy/Validation\")\n",
    "\n",
    "VideoClassifier.val_dataloader = val_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_step(self, batch, batch_idx):\n",
    "    \"\"\"\n",
    "    Validation steps\n",
    "    \"\"\"\n",
    "    # Extractino of data and target\n",
    "    video, label = batch[\"video\"], batch[\"label\"]\n",
    "    # Forward pass\n",
    "    out = self(video)\n",
    "    # Calculation of loss and metric\n",
    "    loss = self.criterion(out.squeeze(), label.float())\n",
    "    metric = self.metric(out, label.to(torch.int64))\n",
    "\n",
    "    # logging the loss\n",
    "    self.log(\"val_loss\", loss)\n",
    "\n",
    "    self.validation_step_loss.append(loss)\n",
    "    self.validation_step_metric.append(metric)\n",
    "\n",
    "    return {\"val_loss\": loss, \"metric\": metric.detach()}\n",
    "\n",
    "VideoClassifier.validation_step = validation_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_validation_epoch_end(self, *args, **kwargs):\n",
    "    \"\"\"\n",
    "    Whenever the validation ends this function runs.\n",
    "    \"\"\"\n",
    "    epoch_avg_loss = torch.stack(self.validation_step_loss).mean()\n",
    "    epoch_avg_metric = torch.stack(self.validation_step_metric).mean()\n",
    "\n",
    "    self.log(\"Average Validation Loss\", epoch_avg_loss)\n",
    "    self.log(\"Average Validation Accuracy\", epoch_avg_metric)\n",
    "\n",
    "    self.validation_step_loss.clear()\n",
    "    self.validation_step_metric.clear()\n",
    "\n",
    "VideoClassifier.on_validation_epoch_end = on_validation_epoch_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_dataloader(self):\n",
    "    \"\"\"\n",
    "    loader for test set.\n",
    "    \"\"\"\n",
    "    return data_loader(path=\"Dataset/Dummy/Test\")\n",
    "\n",
    "VideoClassifier.test_dataloader = test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test_step(self, batch, batch_idx):\n",
    "    \"\"\"\n",
    "    Testing step\n",
    "    \"\"\"\n",
    "    # Extracting the data and label\n",
    "    video, label = batch[\"video\"], batch[\"label\"]\n",
    "\n",
    "    # Forward pass\n",
    "    pred = self(video)\n",
    "\n",
    "    # Calculation of loss\n",
    "    loss = self.criterion(pred.squeeze(), label.float())\n",
    "\n",
    "    self.log(\"test_loss\", loss)\n",
    "    self.log(\"length pred\", len(pred))\n",
    "    \n",
    "    treshold = 0.05\n",
    "\n",
    "    # Transfering to CPU \n",
    "    pred = pred.detach().cpu()\n",
    "    label = label.detach().cpu()\n",
    "    \n",
    "    # Calculation of accuracy, f1_score, precision, recall\n",
    "    accuracy = functional.accuracy(pred, label, task=\"binary\")\n",
    "    f1_score_pred = functional.f1_score(pred, label, task=\"binary\", average=\"weighted\", threshold=treshold)\n",
    "    confmat = functional.confusion_matrix(pred, label, task=\"binary\")\n",
    "    precision = functional.precision(pred, label, task=\"binary\", threshold=treshold)\n",
    "    recall = functional.recall(pred, label, task=\"binary\", threshold=treshold)\n",
    "\n",
    "    self.log(\"Precision\",precision)\n",
    "    self.log(\"Recall\",recall)\n",
    "    self.log(\"test_accuracy\", accuracy)\n",
    "    self.log(\"train_f1\", f1_score_pred)\n",
    "    \n",
    "    self.validation_step_y_hats.append(pred)\n",
    "    self.validation_step_ys.append(label)\n",
    "        \n",
    "    return {\"preds\": pred, \"targets\": label}\n",
    "\n",
    "VideoClassifier.test_step = test_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def on_test_epoch_end(self):\n",
    "    \"\"\"\n",
    "    Whenever the test is performed, this function runs.\n",
    "    \"\"\"\n",
    "    pred = torch.cat(self.validation_step_y_hats)\n",
    "    label = torch.cat(self.validation_step_ys)\n",
    "\n",
    "    confusion_matrix = torchmetrics.ConfusionMatrix(task=\"binary\", num_classes=2, threshold=0.05)\n",
    "    confusion_matrix(pred, label.int())\n",
    "\n",
    "    confusion_matrix_computed = confusion_matrix.compute().detach().cpu().numpy().astype(int)\n",
    "\n",
    "    df_cm = pd.DataFrame(confusion_matrix_computed)\n",
    "    plt.figure(figsize = (10, 7))\n",
    "    fig_ = sns.heatmap(df_cm, annot=True, cmap=\"Spectral\").get_figure()\n",
    "    plt.close(fig_)\n",
    "    \n",
    "    self.loggers[0].experiment.add_figure(\"Confusion matrix\", fig_, self.current_epoch)\n",
    "\n",
    "VideoClassifier.on_test_epoch_end = on_test_epoch_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Checkpointing\n",
    "checkpoint_callback = ModelCheckpoint(monitor=\"val_loss\", dirpath=\"checkpoints\",\n",
    "                                      filename=\"file\", save_last=True)\n",
    "lr_monitor = LearningRateMonitor(logging_interval=\"epoch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 1\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "# Creating and training the model\n",
    "\n",
    "# Initializing the model\n",
    "model = VideoClassifier()\n",
    "\n",
    "# seeding for reproducibility\n",
    "seed_everything(1)\n",
    "\n",
    "# Initializing the trainer\n",
    "trainer = Trainer(min_epochs=10,\n",
    "                  max_epochs=15,\n",
    "                  accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
    "                  devices=-1 if torch.cuda.is_available() else 1,\n",
    "                  precision=\"16-mixed\",\n",
    "                  accumulate_grad_batches=2,\n",
    "                  enable_checkpointing=True,\n",
    "                  enable_progress_bar=True,\n",
    "                  enable_model_summary=True,\n",
    "                  num_sanity_val_steps=0,\n",
    "                  callbacks=[lr_monitor, checkpoint_callback],\n",
    "                  limit_predict_batches=5,\n",
    "                  limit_val_batches=5,\n",
    "                  limit_test_batches=5\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA GeForce RTX 3050 Ti Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "Missing logger folder: c:\\Users\\LEGION\\Documents\\My Documents\\KU\\First Semester\\Data Analytics\\Video Classification using 3D Convolution\\lightning_logs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name         | Type              | Params\n",
      "---------------------------------------------------\n",
      "0 | video_module | Video3DCNN        | 1.9 M \n",
      "1 | relu         | ReLU              | 0     \n",
      "2 | linear       | Linear            | 401   \n",
      "3 | metric       | BinaryAccuracy    | 0     \n",
      "4 | criterion    | BCEWithLogitsLoss | 0     \n",
      "---------------------------------------------------\n",
      "1.9 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 M     Total params\n",
      "7.479     Total estimated model params size (MB)\n",
      "c:\\Users\\LEGION\\anaconda3\\envs\\rl\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\LEGION\\anaconda3\\envs\\rl\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: |          | 20/? [00:45<00:00,  0.44it/s, v_num=0]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LEGION\\anaconda3\\envs\\rl\\lib\\site-packages\\pytorch_lightning\\utilities\\data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 5. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: |          | 20/? [00:42<00:00,  0.47it/s, v_num=0]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=15` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: |          | 20/? [00:42<00:00,  0.47it/s, v_num=0]\n"
     ]
    }
   ],
   "source": [
    "# Training the model\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "c:\\Users\\LEGION\\anaconda3\\envs\\rl\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation DataLoader 0:  40%|████      | 2/5 [00:00<00:00,  4.30it/s]\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "      Validate metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Average Validation Accuracy    0.6000000238418579\n",
      "  Average Validation Loss       1.151508092880249\n",
      "         val_loss               1.151508092880249\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    }
   ],
   "source": [
    "# Checking the validation result\n",
    "val_res = trainer.validate(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "c:\\Users\\LEGION\\anaconda3\\envs\\rl\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:441: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0:  40%|████      | 2/5 [00:00<00:01,  2.65it/s]\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        Precision                  0.25\n",
      "         Recall                     0.5\n",
      "       length pred                  5.0\n",
      "      test_accuracy         0.4000000059604645\n",
      "        test_loss           1.6746532917022705\n",
      "        train_f1            0.3333333432674408\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 1.6746532917022705,\n",
       "  'length pred': 5.0,\n",
       "  'Precision': 0.25,\n",
       "  'Recall': 0.5,\n",
       "  'test_accuracy': 0.4000000059604645,\n",
       "  'train_f1': 0.3333333432674408}]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the test result\n",
    "trainer.test(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Saving the model\n",
    "def save_model_pickle(model, filepath):\n",
    "    with open(filepath, 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "\n",
    "\n",
    "# Save the model\n",
    "save_model_pickle(model, \"models/3DCNN_video_classifier_v1.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorchvideo.data.encoded_video import EncodedVideo\n",
    "\n",
    "# Predictin the realword case scenario usecase\n",
    "# play_video(\"Dataset/Real Life Violence Dataset/Validation/Violence/V_436.mp4\")\n",
    "\n",
    "# video = EncodedVideo.from_path(\"Dataset/Real Life Violence Dataset/Validation/Violence/V_436.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_video(\"Dataset/Real Life Violence Dataset/Validation/NonViolence/NV_267.mp4\")\n",
    "\n",
    "video = EncodedVideo.from_path(\"Dataset/Real Life Violence Dataset/Validation/NonViolence/NV_267.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 10, 112, 112])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transforming the video\n",
    "video_data = video.get_clip(0, 2)\n",
    "video_data = video_transform(video_data)\n",
    "video_data[\"video\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 10, 112, 112])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transferming the model to GPU\n",
    "model = model.cuda()\n",
    "\n",
    "# Transfering the input to GPU\n",
    "inputs = video_data[\"video\"].cuda()\n",
    "\n",
    "# Adding Dimension as the input must be in form (batch_size, number_of_channel, no_of_frame, width, height)\n",
    "inputs = torch.unsqueeze(inputs, 0) # adding dimensions\n",
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.8530151], dtype=float32)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predicting the result\n",
    "preds = model(inputs)\n",
    "preds = preds.detach().cpu().numpy()\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.8530151"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Probability of the prediction\n",
    "preds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converting the probability to class\n",
    "preds = np.where(preds>0.5, 1, 0)\n",
    "preds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the VSCode has tensorboard extension, run this to start the tensorboard\n",
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir lightning_logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Logs are generated in tensorboard: to run tensorboard run <strong>tensorboard --logdir lightning_logs</strong>_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
