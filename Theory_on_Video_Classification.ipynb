{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video Classification using 3D CNN\n",
    "\n",
    "3D Convolutional Neural Networks (3D CNNs) enhance video classification by processing spatial and temporal dimensions, capturing motion and context across multiple frames. This enables effective action recognition, gesture detection, and video surveillance. Despite their computational demands and the need for large datasets, 3D CNNs offer superior performance and simplified pipelines. As technology advances, the adoption of 3D CNNs in video classification will continue to drive innovation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color: orangered\">Some Brief About Video Classification</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a 4-dimensional tensor: T x 3 x H x W ( or 3 x T x H x W). Where:\n",
    "\n",
    "- T: is the time dimension\n",
    "- 3: corresponds to the RGB channels of an image.\n",
    "- H: height of the image.\n",
    "- W: width of the image.\n",
    "\n",
    "In this way we have 1 temporal dimension and 3 space dimensions. Whereas in object recognition tasks we want to classify objects, in video classification we usually want to classify actions or activities.\n",
    "\n",
    "<div align=\"center\"><img src=\"https://i.imgur.com/JjcYk6V.png\" style=\"width: 50%\"></div>\n",
    "\n",
    "__Problem: Videos are Big!__\n",
    "\n",
    "Most common day videos are really big. Videos are usually filmed at around 30 fps (frames per second), this means that each video second contains 30 images. Also, videos are filmed in high resolutions so for example an HD (1920 x 1080) video weighs ~10 GB per minute! By seeing these numbers we can see that is unfeasible to build deep neural networks that can process this amount of data.\n",
    "\n",
    "__Solution__: reduce the amount of data by training on short clips with low fps and low spatial resolution. For example, T=16, H=W=112 (3.2 seconds at 5 fps. 588 KB)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### __Different Approaches to tackle video classification__\n",
    "\n",
    "##### Single-Frame CNN\n",
    "\n",
    "In this approach we pass each frame of a video into a 2D CNN an average predictions using an average mechanism to get the final prediction for the whole video.\n",
    "\n",
    "This model seems to completely ignore the temporal relation between frames but actually happens to be a very very strong baseline!\n",
    "\n",
    "<div align=\"center\"><img src=\"https://i.imgur.com/6XnzCMr.png\" style=\"width: 50%\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### Late Fusion\n",
    "\n",
    "In Single-Frame model we run a 2D CNN over each frame of videos and averaged the predictions using some averaging mechanism. In the Late Fusion approach we will run each frame independently to get an embedding as an output feature and later concatenate the output features and feed them to a Multi Layer Perceptron that handles classification.\n",
    "\n",
    "<div align=\"center\"><img src=\"https://i.imgur.com/fPMstMz.png\" style=\"width: 50%\"></div>\n",
    "\n",
    "However, using Fully Connected Layers add a lot of learnable parameters to a network and may lead to overfitting. One way to tackle this problem is using global average pooling over space and time and later use a smaller FCL.\n",
    "\n",
    "<div align=\"center\"><img src=\"https://i.imgur.com/J85BNY2.png\" style=\"width: 50%\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Early Fusion\n",
    "\n",
    "In Late Fusion we were independently processing all the video frames and then fusing temporal information at the end of the network. With Early Fusion we fuse all temporal information at the beggining of the network by reshaping data as a 3D tensor where we stack all frames along the channel dimension. Basically, we are concatenating all frames along the channel dimension and then fuse all this temporal information using a single 2D convolutional operator.\n",
    "\n",
    "<div align=\"center\"><img src=\"https://i.imgur.com/w9K1ev7.png\" style=\"width: 50%\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Slow Fusion\n",
    "\n",
    "The problem with the Early Fusion approach it is that it may be too aggressive in the way that we pool or aggregate the temporal information because we are \"destroying\" all the temporal information after one convolution layer and one convolution layer would not be enough to model all temporal interactions that happen in a video sequence. We want a mechanism that does not fuse early or late but rather slowly along temporal information.\n",
    "\n",
    "We can use a 3D CNN where in each layer we can maintain 4D tensors. In each layer of the CNN we will use 3-dimensional convolutions and poolings that will allow us to fuse information slowly over the course of many layers of processing.\n",
    "\n",
    "<div align=\"center\"><img src=\"https://i.imgur.com/VkfxDGK.png\" style=\"width: 50%\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Extra Resource to understand about the video classification - [link](https://www.youtube.com/watch?v=A9D6NXBJdwU&list=PL5-TkQAfAZFbzxjBHtzdVCWE0Zbhomg7r&index=18)_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
